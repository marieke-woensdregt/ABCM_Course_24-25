{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45536470",
   "metadata": {},
   "source": [
    "# ABCM Computer lab 1: Social coordination & Theory of mind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670194e2",
   "metadata": {},
   "source": [
    "In this computer lab, we will use the ```tomsup``` package created by Waade et al. (2022) to simulate various Game Theory games between agents that have varying levels of theory of mind (or other strategies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d23236",
   "metadata": {},
   "source": [
    "If you are completely new to Jupyter notebooks, start with this introduction:\n",
    "[https://realpython.com/jupyter-notebook-introduction/](https://realpython.com/jupyter-notebook-introduction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03602bd0",
   "metadata": {},
   "source": [
    "All exercises are indicated with an **Exercise _N_** header. The notebook also contains some explanation, which is interleaved with small coding exercises (of the form _\"In the code cell below, do X\"_) which help you understand how the code works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99bde04",
   "metadata": {},
   "source": [
    "You can find the documentation of the ```tomsup``` package here: https://kennethenevoldsen.github.io/tomsup/index.html\n",
    "\n",
    "**Have a quick browse through the documentation**, so that you know what information you can find there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc56f7a",
   "metadata": {},
   "source": [
    "Below, we are going to install some specific versions of a couple of packages (see the requirements.txt file). Notably, the version of ```pandas``` that needs to be installed in order for the ```tomsup``` package to work is an older one (version 1.3.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d90d5",
   "metadata": {},
   "source": [
    "If you want, you could first install a virtual environment for these computer labs, so that the specific versions of packages we need to install do not interfere with your other Python environment(s).\n",
    "\n",
    "You can find instructions on how to create a virtual environment using ```virtualenv``` and run it in Jupyter notebooks under the link below. If you prefer to create your virtual environment in another way, for example using Anaconda, feel free to do so.\n",
    "**Note** that you have to run those commands in the terminal, *not* from inside a Jupyter notebook. \n",
    "And also **note** that you should create the virtual environment in the same folder in which you have this Jupyter notebook saved.\n",
    "Find the instructions for creating a virtual environment and running a Jupyter notebook inside that environment here: https://medium.com/@WamiqRaza/how-to-create-virtual-environment-jupyter-kernel-python-6836b50f4bf4\n",
    "\n",
    "\n",
    "\n",
    "Once you've created your virtual environment, don't forget to change the kernel to that virtual environment from inside this Jupyter notebook, by going to Kernel --> Change kernel --> ```<myenv>```, where ```<myenv>``` is the name of your virtual environment. You can check in the top right corner of your notebook whether you now indeed see the name of your virtual environment (to the left of the grey circle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554c7a8",
   "metadata": {},
   "source": [
    "First, let's install all the required versions of the required packages by running the code cell below. \n",
    "\n",
    "**Note:** Before running the cell below, make sure that the file requirements.txt is saved in the same folder from which you are running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c10db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T13:12:44.320363Z",
     "start_time": "2024-09-16T13:12:42.901616Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d658e",
   "metadata": {},
   "source": [
    "Now, let's do the necessary imports by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d333c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T13:12:48.209540Z",
     "start_time": "2024-09-16T13:12:46.770270Z"
    }
   },
   "outputs": [],
   "source": [
    "import tomsup as ts\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781c16f",
   "metadata": {},
   "source": [
    "Here are a few things to keep in mind throughout the computer lab:\n",
    "- When plotting results, pay attention to the scale of the y-axis.\n",
    "- When running simulations, make sure that you run:\n",
    "    1. Enough rounds such that the agents' behaviour and/or estimates of each other's parameters is no longer changing (i.e., that you run the simulation until convergence).\n",
    "    2. Enough independent simulation runs to get a good sense of the stochastic variation between runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6b8a7",
   "metadata": {},
   "source": [
    "## Exploring the games in tomsup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c887a",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "\n",
    "Use the command ```help(ts.PayoffMatrix)``` (see page 11 of Waade et al., 2022) to explore what Game Theory games are pre-specified in the tomsup package. Print and investigate each of these pay-off matrices. For each one: Write down whether they are competitive or cooperative in nature. Also explain why.\n",
    "\n",
    "**Note** that there's a typo in one of the game names in the documentation of ```tomsup```. It should be ```'penny_competitive'```, _not_ ```'penny_competive'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86651d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f243f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a35055",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1887183",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "\n",
    "```penny_competitive``` is an example of a zero-sum game. The definition of a zero-sum game is as follows:\n",
    "_\"games in which choices by players can neither increase nor decrease the available resources. In zero-sum games, the total benefit that goes to all players in a game, for every combination of strategies, always adds to zero (more informally, a player benefits only at the equal expense of others)\"_\n",
    "Can you find any other example of a zero-sum game among the predefined games in the tomsup package?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc87d9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a5f1008",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "\n",
    "```prisoners_dilemma``` is an example of a game that has a Nash equilibrium that is suboptimal for both agents. That is, when both agents decide to betray each other (i.e, both choose action 0), they are worse off than if they both remain silent (i,e., both choose action 1). However, if they are in a state where they both choose action 0, neither agent can improve their own pay-off by changing strategy, making this state a Nash equilibrium. \n",
    "Can you find any other games among the predefined games that have such a Nash equilibrium that is suboptimal for both agents? If so, explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39707a8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce83a298",
   "metadata": {},
   "source": [
    "## Running interactions between agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a5588",
   "metadata": {},
   "source": [
    "### Creating a game:\n",
    "A game can be created using the ```PayoffMatrix``` class, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289d254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T12:36:40.284430Z",
     "start_time": "2024-09-04T12:36:40.280520Z"
    }
   },
   "outputs": [],
   "source": [
    "penny = ts.PayoffMatrix(name='penny_competitive')\n",
    "\n",
    "print(penny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceee5e6",
   "metadata": {},
   "source": [
    "Try this in the code cell below by creating a staghunt game and printing it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e65510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc72bcc6",
   "metadata": {},
   "source": [
    "### Creating a group of agents\n",
    "\n",
    "A group of agents can be created quickly, using the ```create_agents()``` function, which returns an object of the ```AgentGroup``` class. This function takes two input arguments:\n",
    "1. ```agents```: specifies the agent types in the group. Possible agent types are:\n",
    "    - 'RB'\n",
    "    - 'QL'\n",
    "    - 'WSLS'\n",
    "    - '1-TOM'\n",
    "    - '2-TOM'\n",
    "2. ```start_params```: specifies the starting parameters for each agent. An empty dictionary, denoted by ```{}```, gives default values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0776c",
   "metadata": {},
   "source": [
    "In the code cell below, use the ```ts.create_agents()``` function to create an object of the ```AgentGroup``` class, which you assign to a variable called ```group```.\n",
    "Use the following input arguments:\n",
    "1. Create a list called ```agent_types``` which contains all possible agent types names as listed above. Pass that as the ```agents``` argument\n",
    "2. Create a list called ```starting_parameters``` which contains:\n",
    "    - ```{'bias':0.7}``` for the 'RB' agent\n",
    "    - ```{'learning_rate':0.5}``` for the 'QL' agent\n",
    "    - the default parameters (i.e., empty dictionary) for all other agent types\n",
    "\n",
    "Once you've created your ```group``` object, print it and inspect it, using ```print(group)``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2120944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa9faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8580a772",
   "metadata": {},
   "source": [
    "You can inspect the further functionality of the ```AgentGroup``` class using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632db9cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T12:36:49.319413Z",
     "start_time": "2024-09-04T12:36:49.309589Z"
    }
   },
   "outputs": [],
   "source": [
    "help(ts.AgentGroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e1473",
   "metadata": {},
   "source": [
    "### Setting the type of interaction\n",
    "\n",
    "The ```.set_env()``` method of the AgentGroup class allows you to set the type of 'tournament' that the agents will interact in. The possible strings that can be passed to the ```env``` input argument are:\n",
    "- 'round_robin': Matches all agents against all others\n",
    "- 'random_pairs': Combines the agents in random pairs (the number of agents must be even)\n",
    "\n",
    "Assuming you have now created an ```AgentGroup``` object called ```group```, the code cell below shows you how to set the environment to ```'round_robin'```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109180c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T12:36:52.820608Z",
     "start_time": "2024-09-04T12:36:52.552486Z"
    }
   },
   "outputs": [],
   "source": [
    "group.set_env(env='round_robin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c711741",
   "metadata": {},
   "source": [
    "### Running a tournament\n",
    "\n",
    "The ```.compete()``` method of the AgentGroup class allows you to run a competition between the agents of the type that you've specified using the ```.set_env()``` method.\n",
    "\n",
    "Assuming you have now created an ```AgentGroup``` object called ```group```, the code cell below shows you how to run a tournament of the 'penny_competitive' game, where the group cometes for 50 simulations of 50 rounds (note that this takes a little while to run). The ```.compete()``` method returns a Pandas dataframe containing various results of the tournament. Below, this dataframe is saved in a variable called ```results```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2198c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-04T12:37:11.333463Z",
     "start_time": "2024-09-04T12:37:11.323335Z"
    }
   },
   "outputs": [],
   "source": [
    "results = group.compete(p_matrix='penny_competitive', n_rounds=50, n_sim=50, save_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067fa36",
   "metadata": {},
   "source": [
    "Assuming the tournament has finished running, we can have a look at the structure of the ```results``` dataframe using the ```.head``` attribute of the Pandas dataframe, which gets the first 5 and last 5 rows of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cc7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T14:16:56.131563Z",
     "start_time": "2022-09-20T14:16:56.112569Z"
    }
   },
   "outputs": [],
   "source": [
    "print(results.head) # print the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e66fcd9",
   "metadata": {},
   "source": [
    "### Plotting heatmap of tournament results\n",
    "\n",
    "The ```AgentGroup``` class also comes with a number of plotting methods (use ```help(ts.AgentGroup)``` to inspect). The ```.plot_heatmap()``` method creates a heatmap of the rewards of all agents in the tournament, similar to Figure 3 (p. 17) in Waade et al. (2022). The code cell below demonstrates how to using this method (assuming the tournament has finished running):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe7bedc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T14:17:00.435396Z",
     "start_time": "2022-09-20T14:17:00.071568Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [10, 10] # Set figure size\n",
    "plt.title(\"All agents playing penny_competitive\") # Set figure title\n",
    "group.plot_heatmap(cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da505e2",
   "metadata": {},
   "source": [
    "This heatmap displays the average score across simulations for each competing pair. The score denotes the score of the agent (x-axis) when playing against the opponent (y-axis). The score in the parenthesis denotes the 95% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a320ac0",
   "metadata": {},
   "source": [
    "**Exercise 4:**\n",
    "\n",
    "In the ```'penny_competitive'``` game, the ```'2-TOM'``` agent usually has a slight advantage over the ```'1-TOM'``` agent when they play against each other (see top-left corner of the heatmap).\n",
    "Write some code that focuses on interactions between ```'1-TOM'``` and ```'2-TOM'``` agents, in order to find out whether there are games among the predefined games in ```tomsup``` for which this is the other way around. That is, where the ```'1-TOM'``` agent outperforms the ```'2-TOM'``` agent when they interact with each other.\n",
    "\n",
    "**Tip 1:** The ```.compete()``` method does a lot of printing as it's running. If this is getting in your way, you can switch off the printing by setting the ```verbose``` input argument of the ```.compete()``` method to ```False```.\n",
    "\n",
    "**Tip 2:** You can give a title to a figure using ```plt.title(\"my_title_string\")```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915525a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c5923c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0dc41b5",
   "metadata": {},
   "source": [
    "### Plotting agents' scores over rounds:\n",
    "\n",
    "The ```AgentGroup``` class comes with a method ```.plot_score()``` which allows you to plot how the scores of the agents change over rounds (as they're learning about each other). Below is an example for how to use this method to plot the scores over time of the ```'1-TOM'``` agent when playing against the ```'2-TOM'``` agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5fa264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T09:59:07.591810Z",
     "start_time": "2022-09-20T09:59:07.280683Z"
    }
   },
   "outputs": [],
   "source": [
    "group.plot_score(agent0=\"1-TOM\", agent1=\"2-TOM\", agent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080a88b",
   "metadata": {},
   "source": [
    "As you can see, the ```1-TOM``` agent's mean score doesn't change much over time, but the individual simulations do start differing more from each other over rounds. In the text box below, write down your thoughts about what could be the cause of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec49ba",
   "metadata": {},
   "source": [
    "### Plotting agent choices over rounds:\n",
    "\n",
    "The ```AgentGroup``` class also comes with a method ```.plot_choice()``` which allows you to plot the choices of the agents across rounds. Below is an example for how to use this method to plot the choices of the ```'1-TOM'``` agent against those of the ```'2-TOM'``` agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de6a928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T09:19:37.403523Z",
     "start_time": "2022-09-20T09:19:37.276387Z"
    }
   },
   "outputs": [],
   "source": [
    "group.plot_choice(agent0=\"1-TOM\", agent1=\"2-TOM\", agent=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee384d",
   "metadata": {},
   "source": [
    "### Plotting _k_-ToM agent's estimate of other agent's sophistication level (_k_):\n",
    "\n",
    "The ```AgentGroup``` class also comes with a method ```.plot_p_k()``` which allows you to plot a given _k_-ToM agent's estimate of what the other agent's level of _k_ is, over rounds. Below is an example for how to use this method to plot the probability that the ```'2-TOM'``` agent assigns to the possibility that the ```'1-TOM'``` agent that they are playing against has sophistication level of _k_=1, over rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba36e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-20T10:01:01.039870Z",
     "start_time": "2022-09-20T10:01:00.697435Z"
    }
   },
   "outputs": [],
   "source": [
    "# The agent input argument specifies which agent's estimate should be shown (agent0 or agent1)\n",
    "# The level input argument specifies for which level of k the probability should be shown over rounds\n",
    "\n",
    "group.plot_p_k(agent0=\"1-TOM\", agent1=\"2-TOM\", agent=1, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daff84cf",
   "metadata": {},
   "source": [
    "**Exercise 5:**\n",
    "\n",
    "Choose one of the games that came out of Exercise 4 as a very clear example of a game where the ```1-TOM``` agent has a significant advantage over the ```2-TOM``` agent. For this particular game, **try to figure out why this might be the case**.\n",
    "\n",
    "Good first steps towards figuring this out are:\n",
    "1. Inspect the pay-off matrix of the game in question\n",
    "2. Plot the scores of the two agents over rounds **when playing the game in question**\n",
    "3. Plot the choices that the two agents make **when playing the game in question**\n",
    "4. Plot the agents' estimates of each other's sophistication levels (_k_) **when playing the game in question**\n",
    "\n",
    "With each plot that you make in order to answer this question, also add some text to explain what you see happening in the different plots, and what that means (relevant to the answer to this question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d825b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa36946a",
   "metadata": {},
   "source": [
    "**BONUS Exercise 6 (only if you have time left):**\n",
    "\n",
    "Continuing with the same game you inspected for Exercise 5, have a look at what happens when the ```1-TOM``` and ```2-TOM``` agent interact with the 'random bias' agent ('RB').\n",
    "\n",
    "Use the ```.plot_tom_op_estimate()``` of the ```AgentGroup``` class to inspect how the ```2-TOM``` agent estimates the ```RB``` agent's bias over time. Does the ```2-TOM``` agent reach the accurate inference eventually? You may want to run more rounds to make sure that the model has converged (i.e., that the ```2-TOM``` agent's estimate of the bias is no longer changing). \n",
    "\n",
    "With each plot that you make in order to answer this question, also add some text to explain what you see happening in the different plots, and what that means (relevant to the answer to this question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2916fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dfbbd7c",
   "metadata": {},
   "source": [
    "If you're working in a virtual environment, **don't forget to deactivate your virtual environment** using the ```deactivate``` command."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abcm",
   "language": "python",
   "name": "abcm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
